{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import random\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mnist_df = pd.read_csv(\"ex3_train.csv\")\n",
    "# Alternative way of doing -----Mnist_df_train_data = Mnist_df.as_matrix(columns=Mnist_df.columns[0:400])\n",
    "Mnist_df_train_data = Mnist_df.values\n",
    "Mnist_df_train_label = Mnist_df.as_matrix(columns=Mnist_df.columns[400:401])\n",
    "Mnist_df_train_data_with_bias = np.c_[np.ones((Mnist_df_train_data.shape[0] ,1)),Mnist_df_train_data]## Add 1 for the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mnist_df_test = pd.read_csv(\"ex3_test.csv\")\n",
    "Mnist_df_test_data = Mnist_df_test.values\n",
    "Mnist_df_test_label = Mnist_df_test.as_matrix(columns=Mnist_df.columns[400:401])\n",
    "Mnist_df_test_data_with_bias = np.c_[np.ones((Mnist_df_test_data.shape[0] ,1)),Mnist_df_test_data]## Add 1 for the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111f3a588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(1,1))\n",
    "plt.imshow(np.reshape(Mnist_df_train_data[:,:-1][2],[20,20]).T,cmap='gray')\n",
    "print (Mnist_df_train_data[2][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Label set - containing unique labels\n",
    "Mnist_df_train_label_uq = set(Mnist_df.values[:,400].astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Not Being used for this assignment\n",
    "def hot_encoding_dict(label_set):\n",
    "    identity_matrix = np.identity(len(label_set)).astype(int)\n",
    "    hot_encoding_dict = {v:identity_matrix[:,i] for i,v in enumerate(label_set)}\n",
    "    return hot_encoding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Being used for this assignment\n",
    "def label_dict_gen(label_set): ## Useful for scaling if  huge number of labels (both number and string)\n",
    "    label_dict = {}\n",
    "    count = 0\n",
    "    for label in label_set:\n",
    "        label_dict[label]=count\n",
    "        count +=1\n",
    "    return label_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Not Being used for this assignment - better method exists for calculating the cross entropy loss\n",
    "## Nielsen Deep Learning applicable for number labels ,if input is string label enumerate the labels and then use the  \n",
    "## enumerated index below \n",
    "def hot_encoding_lower_memory_footprint(label,number_of_labels):\n",
    "    e = np.zeros((number_of_labels, 1))\n",
    "    e[label] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Not Being used for this assignment - better method exists for calculating the cross entropy loss\n",
    "## Refer - corect_logprobs = -np.log(softmax_output[range(softmax_output.shape[0]),label_general])## Andrej Karpathy Course\n",
    "def prepare_true_label_hot_encoding_array(label_dict,labels):\n",
    "    label_list = []\n",
    "    for label in labels:\n",
    "        label_list.append(hot_encoding_lower_memory_footprint(label_dict[label],len(label_dict)))\n",
    "    label_list = np.array(label_list)\n",
    "    return label_list.reshape(-1,len(label_dict)).T\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print (prepare_true_label_hot_encoding_array(label_dict(Mnist_df_train_label_uq),[0,0,1,9,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(data):\n",
    "    return 1/(1+np.exp(-data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_prediction(data):\n",
    "    result= np.exp(data) / np.sum(np.exp(data), axis=1, keepdims=True)\n",
    "    return result ## The max proability per coloumn(each coloumn represents a record , and \n",
    "                                         ## The complete result matrix to calculate the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize weights along with weights for bias set to zero\n",
    "def weights_list_generator(features,hidden_layers_list,no_output_class):\n",
    "    np.random.seed(1)\n",
    "    weights_array=[]\n",
    "    for i in range(len(hidden_layers_list)):\n",
    "        if i == 0 :\n",
    "            weights_array=[.01*np.random.randn(features,hidden_layers_list[0])]\n",
    "           ## Add row of zeroes for bias term's initial weights (initialized to zero)\n",
    "            weights_array[0] = np.r_[np.zeros(( 1,hidden_layers_list[0])),weights_array[0]]\n",
    "        else :\n",
    "            weights_array.append(.01*np.random.randn(hidden_layers_list[i-1],hidden_layers_list[i]))\n",
    "            ## Add row of zeroes for bias term's initial weights (initialized to zero)\n",
    "            weights_array[i] = np.r_[np.zeros(( 1,hidden_layers_list[i])),weights_array[i]]\n",
    "    weights_array.append(.01*np.random.randn(hidden_layers_list[-1],no_output_class))\n",
    "    weights_array[-1] = np.r_[np.zeros(( 1,no_output_class)),weights_array[-1]]\n",
    "    return weights_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights_list_try = weights_list_generator(400,[25],10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights_list_try[1][1:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights_list_try[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nweights_list = weights_list_generator(400,[25],10)\\nhidden_input = np.dot(Mnist_df_train_data_with_bias[:,:-1],weights_list[0])\\nhidden_output = sigmoid(hidden_input)\\nhidden_output = np.c_[np.ones((hidden_output.shape[0] ,1)),hidden_output]\\n#print ((1-hidden_output))\\n#print ((1-hidden_output[:,1:]))\\n#print (hidden_output[:,1:].shape)\\noutput_before_softmax = np.dot(hidden_output,weights_list[1])\\n#print(output_before_softmax)\\n#print(output_before_softmax.shape)\\nsoftmax_output = softmax_prediction(output_before_softmax)\\n#print(softmax_output)\\n#print(softmax_output.shape)\\nlabel_dict = label_dict_gen(Mnist_df_train_label_uq)\\nlabel_general = [label_dict[label] for label in Mnist_df_train_data_with_bias[:,-1]]\\nlabel_general = np.asarray(label_general)\\ncorect_logprobs = -np.log(softmax_output[range(softmax_output.shape[0]),label_general])\\n#print (corect_logprobs)\\n#print (corect_logprobs.shape)\\n## Sum the individual terms of output of autoencoding snd softmax in the above step and average it\\nloss = np.sum(corect_logprobs)/softmax_output.shape[0]    \\n\\n# compute the gradient on scores\\ndsoftmax = softmax_output\\ndsoftmax[range(softmax_output.shape[0]),label_general] -= 1 ## Very elegant only the element with true label is updated\\n#print (dsoftmax)\\n#print (dsoftmax.shape)\\n# backpropate the gradient to the 2nd layer weight\\ndW1 = np.dot(hidden_output.T, dsoftmax)/softmax_output.shape[0]\\n# perform a parameter update\\n#print (dW1)\\n#print (dW1.shape)\\n#print (weights_list[1])\\ngradient_descent(weights_list[1],dW1,.01)\\n#print (weights_list[1])\\n#print (weights_list[1].shape)\\n# Backpropagate the gradient to 1st layer \\nprint (weights_list[1][1:,:].shape)\\nyo = np.dot(dsoftmax,weights_list[1][1:,:].T)\\n#print(yo)\\n#print(yo.shape)\\npo = np.dot(Mnist_df_train_data_with_bias[:,:-1].T,np.dot(hidden_output[:,1:],(1-hidden_output)[:,1:].T))\\n#print (po)\\n#print (po.shape)\\ndW0 = np.dot(po,yo)/softmax_output.shape[0]\\n#print (dW0)\\n#print (dW0.shape)\\n# perform a parameter update\\ngradient_descent(weights_list[0],dW0,.01)\\nprint(weights_list[0])\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "weights_list = weights_list_generator(400,[25],10)\n",
    "hidden_input = np.dot(Mnist_df_train_data_with_bias[:,:-1],weights_list[0])\n",
    "hidden_output = sigmoid(hidden_input)\n",
    "hidden_output = np.c_[np.ones((hidden_output.shape[0] ,1)),hidden_output]\n",
    "#print ((1-hidden_output))\n",
    "#print ((1-hidden_output[:,1:]))\n",
    "#print (hidden_output[:,1:].shape)\n",
    "output_before_softmax = np.dot(hidden_output,weights_list[1])\n",
    "#print(output_before_softmax)\n",
    "#print(output_before_softmax.shape)\n",
    "softmax_output = softmax_prediction(output_before_softmax)\n",
    "#print(softmax_output)\n",
    "#print(softmax_output.shape)\n",
    "label_dict = label_dict_gen(Mnist_df_train_label_uq)\n",
    "label_general = [label_dict[label] for label in Mnist_df_train_data_with_bias[:,-1]]\n",
    "label_general = np.asarray(label_general)\n",
    "corect_logprobs = -np.log(softmax_output[range(softmax_output.shape[0]),label_general])\n",
    "#print (corect_logprobs)\n",
    "#print (corect_logprobs.shape)\n",
    "## Sum the individual terms of output of autoencoding snd softmax in the above step and average it\n",
    "loss = np.sum(corect_logprobs)/softmax_output.shape[0]    \n",
    "\n",
    "# compute the gradient on scores\n",
    "dsoftmax = softmax_output\n",
    "dsoftmax[range(softmax_output.shape[0]),label_general] -= 1 ## Very elegant only the element with true label is updated\n",
    "#print (dsoftmax)\n",
    "#print (dsoftmax.shape)\n",
    "# backpropate the gradient to the 2nd layer weight\n",
    "dW1 = np.dot(hidden_output.T, dsoftmax)/softmax_output.shape[0]\n",
    "# perform a parameter update\n",
    "#print (dW1)\n",
    "#print (dW1.shape)\n",
    "#print (weights_list[1])\n",
    "gradient_descent(weights_list[1],dW1,.01)\n",
    "#print (weights_list[1])\n",
    "#print (weights_list[1].shape)\n",
    "# Backpropagate the gradient to 1st layer \n",
    "print (weights_list[1][1:,:].shape)\n",
    "yo = np.dot(dsoftmax,weights_list[1][1:,:].T)\n",
    "#print(yo)\n",
    "#print(yo.shape)\n",
    "po = np.dot(Mnist_df_train_data_with_bias[:,:-1].T,np.dot(hidden_output[:,1:],(1-hidden_output)[:,1:].T))\n",
    "#print (po)\n",
    "#print (po.shape)\n",
    "dW0 = np.dot(po,yo)/softmax_output.shape[0]\n",
    "#print (dW0)\n",
    "#print (dW0.shape)\n",
    "# perform a parameter update\n",
    "gradient_descent(weights_list[0],dW0,.01)\n",
    "print(weights_list[0])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(data,weights_list):\n",
    "    hidden_input = np.dot(data[:,:-1],weights_list[0])\n",
    "    hidden_output = sigmoid(hidden_input)\n",
    "    hidden_output = np.c_[np.ones((hidden_output.shape[0] ,1)),hidden_output]## Add 1 for the bias \n",
    "    output_before_softmax = np.dot(hidden_output,weights_list[1])\n",
    "    softmax_output = softmax_prediction(output_before_softmax)\n",
    "    return softmax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(softmax_output,labels):\n",
    "    ## General implementation to take care of any labels\n",
    "    label_general = [label_dict(Mnist_df_train_label_uq)[label] for label in labels]\n",
    "    label_general = np.asarray(label_general)\n",
    "    corect_logprobs = -np.log(softmax_output[range(softmax_output.shape[0]),label_general])## Andrej Karpathy Course\n",
    "    loss = np.sum(corect_logprobs)/softmax_output.shape[0]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Backward_prop(data,initial_weights,label_dict,learning_rate):\n",
    "    hidden_input = np.dot(data[:,:-1],initial_weights[0])\n",
    "    hidden_output = sigmoid(hidden_input)\n",
    "    hidden_output = np.c_[np.ones((hidden_output.shape[0] ,1)),hidden_output]## Add 1 for the bias\n",
    "    #print (\"at_back\",data[:,-1])\n",
    "    output_before_softmax = np.dot(hidden_output,initial_weights[1])\n",
    "    softmax_output = softmax_prediction(output_before_softmax)\n",
    "    label_general = [label_dict[label] for label in data[:,-1]]\n",
    "    label_general = np.asarray(label_general)\n",
    "    #print (\"label\",label_general)\n",
    "    ## Andrej Karpathy Course calcualting the product of softmax with autoencodin to give the cross-entropy array                 \n",
    "    corect_logprobs = -np.log(softmax_output[range(softmax_output.shape[0]),label_general])\n",
    "    ## Sum the individual terms of output of autoencoding snd softmax in the above step and average it\n",
    "    loss = np.sum(corect_logprobs)/softmax_output.shape[0]    \n",
    "    \n",
    "  # compute the gradient on scores\n",
    "    dsoftmax = softmax_output\n",
    "    dsoftmax[range(softmax_output.shape[0]),label_general] -= 1 ## Very elegant only the element with true label is updated\n",
    "\n",
    "  # backpropate the gradient to the 2nd layer weight\n",
    "    dW1 = np.dot(hidden_output.T, dsoftmax)/softmax_output.shape[0]\n",
    "  # perform a parameter update\n",
    "    gradient_descent(initial_weights[1],dW1,learning_rate)\n",
    "    \n",
    "    dlast = np.dot(dsoftmax,initial_weights[1][1:,:].T)\n",
    "    dsigmoid = np.dot(hidden_output[:,1:],(1-hidden_output[:,1:]).T)\n",
    "    \n",
    "    dhidden = np.dot(dsigmoid,dlast)\n",
    "  # Backpropagate the gradient to 1st layer weights\n",
    "    dW0 = np.dot(data[:,:-1].T,dhidden)/softmax_output.shape[0]\n",
    "  # perform a parameter update\n",
    "    gradient_descent(initial_weights[0],dW0,learning_rate)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(weights,gradient,learning_rate):\n",
    "    weights += -learning_rate * gradient\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(training_data, epochs, mini_batch_size,initial_weights,Loss_graph,learning_rate = .0001,test_data=None):\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = training_data.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            np.random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k+mini_batch_size]for k in range(0, n, mini_batch_size)]\n",
    "            #print (len(mini_batches))\n",
    "            for mini_batch in mini_batches:\n",
    "                ## Loss gets updated with each batch and after the loop ends , the loss reflects the loss at end of 1 epoch\n",
    "                Loss = Backward_prop(mini_batch,initial_weights,label_dict,learning_rate)\n",
    "                #print (mini_batch[:,-1])\n",
    "                #break\n",
    "            #break\n",
    "            if epoch % 1000== 0:\n",
    "                Loss_graph[learning_rate].append([Loss,epoch])\n",
    "                print (\"iteration %d: loss %f\" % (epoch, Loss))\n",
    "                softmax_output = forward_prop(training_data,initial_weights)\n",
    "                prediction = np.argmax(softmax_output, axis=1)\n",
    "                label_general = [label_dict_gen(Mnist_df_train_label_uq)[label] for label in training_data[:,-1]]\n",
    "                label_general = np.asarray(label_general)\n",
    "                print(prediction,label_general)\n",
    "                accuracy = np.mean(prediction == label_general)\n",
    "                print ('training accuracy: %.4f' % accuracy)\n",
    "                #if accuracy>0.95  :\n",
    "                    #break\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MINI BATCHING APPROACH\n",
    "# output_batches = []\n",
    "# sample_size = len(features)\n",
    "# for start_i in range(0, sample_size, batch_size):\n",
    "#         end_i = start_i + batch_size\n",
    "#         batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "#         output_batches.append(batch)\n",
    "#     return output_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights = weights_list_generator(400,[25],10)\n",
    "label_dict = label_dict_gen(Mnist_df_train_label_uq)\n",
    "Loss_graph = defaultdict(list)\n",
    "mini_batch_size = 512\n",
    "epochs = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 2.214110\n",
      "[1 2 1 ... 0 1 6] [1 2 1 ... 0 1 6]\n",
      "training accuracy: 0.7854\n",
      "iteration 1000: loss 2.214223\n",
      "[2 0 4 ... 0 2 7] [3 0 4 ... 0 5 7]\n",
      "training accuracy: 0.7874\n",
      "iteration 2000: loss 2.211488\n",
      "[4 2 0 ... 9 2 8] [4 2 0 ... 9 1 8]\n",
      "training accuracy: 0.7974\n",
      "iteration 3000: loss 2.211155\n",
      "[9 8 0 ... 8 2 8] [9 5 0 ... 1 2 4]\n",
      "training accuracy: 0.8026\n",
      "iteration 4000: loss 2.207151\n",
      "[2 8 6 ... 0 8 2] [9 8 6 ... 0 4 1]\n",
      "training accuracy: 0.8117\n",
      "iteration 5000: loss 2.205299\n",
      "[4 4 6 ... 0 7 5] [4 4 6 ... 0 7 0]\n",
      "training accuracy: 0.8137\n",
      "iteration 6000: loss 2.203002\n",
      "[9 0 6 ... 1 2 0] [9 0 6 ... 1 2 0]\n",
      "training accuracy: 0.8211\n",
      "iteration 7000: loss 2.202861\n",
      "[8 5 2 ... 8 9 0] [0 5 2 ... 8 9 0]\n",
      "training accuracy: 0.8237\n",
      "iteration 8000: loss 2.199214\n",
      "[3 4 1 ... 0 5 7] [5 4 1 ... 0 5 7]\n",
      "training accuracy: 0.8277\n",
      "iteration 9000: loss 2.200567\n",
      "[1 5 8 ... 8 3 1] [3 5 8 ... 8 3 1]\n",
      "training accuracy: 0.8314\n",
      "iteration 10000: loss 2.191248\n",
      "[4 4 6 ... 7 7 9] [4 4 6 ... 7 7 9]\n",
      "training accuracy: 0.8383\n",
      "iteration 11000: loss 2.196183\n",
      "[8 7 3 ... 8 3 0] [8 7 5 ... 2 3 0]\n",
      "training accuracy: 0.8383\n",
      "iteration 12000: loss 2.195810\n",
      "[2 3 6 ... 4 2 0] [2 8 6 ... 4 2 0]\n",
      "training accuracy: 0.8434\n",
      "iteration 13000: loss 2.190510\n",
      "[2 2 2 ... 1 1 8] [2 2 2 ... 1 1 9]\n",
      "training accuracy: 0.8397\n",
      "iteration 14000: loss 2.186592\n",
      "[1 0 0 ... 1 8 4] [1 0 0 ... 1 3 4]\n",
      "training accuracy: 0.8486\n",
      "iteration 15000: loss 2.182870\n",
      "[2 7 7 ... 3 7 4] [2 7 7 ... 3 7 4]\n",
      "training accuracy: 0.8511\n",
      "iteration 16000: loss 2.185476\n",
      "[9 5 7 ... 2 6 8] [9 5 7 ... 2 6 8]\n",
      "training accuracy: 0.8494\n",
      "iteration 17000: loss 2.177366\n",
      "[3 1 8 ... 3 8 0] [3 3 8 ... 3 8 0]\n",
      "training accuracy: 0.8509\n",
      "iteration 18000: loss 2.183602\n",
      "[4 0 5 ... 9 1 8] [4 0 5 ... 9 1 3]\n",
      "training accuracy: 0.8571\n",
      "iteration 19000: loss 2.175926\n",
      "[9 9 2 ... 8 0 0] [9 9 3 ... 8 0 0]\n",
      "training accuracy: 0.8571\n",
      "iteration 20000: loss 2.176723\n",
      "[4 1 3 ... 9 1 7] [8 1 3 ... 4 1 9]\n",
      "training accuracy: 0.8651\n",
      "iteration 21000: loss 2.169777\n",
      "[2 5 7 ... 1 1 3] [2 5 7 ... 2 1 3]\n",
      "training accuracy: 0.8614\n",
      "iteration 22000: loss 2.173126\n",
      "[5 6 1 ... 5 1 0] [5 6 1 ... 5 1 0]\n",
      "training accuracy: 0.8657\n",
      "iteration 23000: loss 2.167174\n",
      "[6 9 8 ... 7 4 9] [6 9 8 ... 7 4 9]\n",
      "training accuracy: 0.8651\n",
      "iteration 24000: loss 2.167271\n",
      "[1 7 7 ... 7 8 8] [1 7 7 ... 7 8 8]\n",
      "training accuracy: 0.8706\n",
      "iteration 25000: loss 2.159599\n",
      "[0 8 6 ... 2 2 2] [0 5 6 ... 2 2 2]\n",
      "training accuracy: 0.8674\n",
      "iteration 26000: loss 2.162220\n",
      "[1 1 0 ... 2 8 1] [1 1 0 ... 2 8 1]\n",
      "training accuracy: 0.8663\n",
      "iteration 27000: loss 2.159536\n",
      "[2 7 2 ... 3 8 1] [2 7 1 ... 3 3 1]\n",
      "training accuracy: 0.8720\n",
      "iteration 28000: loss 2.158172\n",
      "[2 1 2 ... 8 9 6] [6 1 8 ... 8 9 6]\n",
      "training accuracy: 0.8689\n",
      "iteration 29000: loss 2.156935\n",
      "[1 8 5 ... 2 3 9] [1 8 9 ... 2 5 9]\n",
      "training accuracy: 0.8711\n",
      "iteration 30000: loss 2.148504\n",
      "[1 7 1 ... 4 0 5] [1 7 1 ... 4 0 3]\n",
      "training accuracy: 0.8734\n",
      "iteration 31000: loss 2.146901\n",
      "[0 4 0 ... 7 1 3] [0 4 0 ... 7 1 3]\n",
      "training accuracy: 0.8691\n",
      "iteration 32000: loss 2.147451\n",
      "[6 5 0 ... 6 9 3] [6 5 0 ... 6 9 5]\n",
      "training accuracy: 0.8697\n",
      "iteration 33000: loss 2.148486\n",
      "[6 7 1 ... 9 4 7] [6 7 2 ... 9 4 7]\n",
      "training accuracy: 0.8737\n",
      "iteration 34000: loss 2.145417\n",
      "[0 4 5 ... 4 5 6] [0 4 5 ... 4 5 6]\n",
      "training accuracy: 0.8766\n",
      "iteration 35000: loss 2.143343\n",
      "[1 4 7 ... 5 7 7] [1 4 7 ... 5 7 7]\n",
      "training accuracy: 0.8729\n",
      "iteration 36000: loss 2.139453\n",
      "[8 0 8 ... 1 6 1] [1 0 5 ... 1 6 1]\n",
      "training accuracy: 0.8754\n",
      "iteration 37000: loss 2.136969\n",
      "[7 6 9 ... 7 1 6] [7 6 9 ... 7 1 6]\n",
      "training accuracy: 0.8791\n",
      "iteration 38000: loss 2.133933\n",
      "[1 7 1 ... 8 4 5] [3 9 7 ... 8 4 5]\n",
      "training accuracy: 0.8777\n",
      "iteration 39000: loss 2.137486\n",
      "[7 0 9 ... 1 9 5] [7 0 9 ... 6 9 5]\n",
      "training accuracy: 0.8766\n",
      "iteration 40000: loss 2.134196\n",
      "[8 0 1 ... 9 3 8] [8 0 1 ... 9 3 1]\n",
      "training accuracy: 0.8777\n",
      "iteration 41000: loss 2.121073\n",
      "[3 3 2 ... 3 4 1] [3 3 2 ... 8 4 1]\n",
      "training accuracy: 0.8694\n",
      "iteration 42000: loss 2.122702\n",
      "[8 9 1 ... 6 5 6] [8 9 1 ... 6 5 6]\n",
      "training accuracy: 0.8734\n",
      "iteration 43000: loss 2.125519\n",
      "[3 1 3 ... 5 9 5] [3 3 5 ... 5 9 5]\n",
      "training accuracy: 0.8803\n",
      "iteration 44000: loss 2.124296\n",
      "[1 5 8 ... 1 7 4] [1 5 8 ... 1 7 4]\n",
      "training accuracy: 0.8737\n",
      "iteration 45000: loss 2.122656\n",
      "[0 4 7 ... 9 6 0] [0 4 7 ... 9 6 0]\n",
      "training accuracy: 0.8751\n",
      "iteration 46000: loss 2.114248\n",
      "[4 9 6 ... 0 1 6] [7 9 6 ... 0 8 8]\n",
      "training accuracy: 0.8746\n",
      "iteration 47000: loss 2.118631\n",
      "[7 1 6 ... 6 1 7] [7 9 6 ... 6 1 7]\n",
      "training accuracy: 0.8743\n",
      "iteration 48000: loss 2.111438\n",
      "[3 8 7 ... 1 3 0] [3 8 7 ... 1 3 0]\n",
      "training accuracy: 0.8709\n",
      "iteration 49000: loss 2.112686\n",
      "[2 8 7 ... 6 4 9] [2 8 7 ... 6 4 9]\n",
      "training accuracy: 0.8806\n"
     ]
    }
   ],
   "source": [
    "SGD(Mnist_df_train_data_with_bias, epochs, mini_batch_size,initial_weights,Loss_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_output_test = forward_prop(Mnist_df_test_data_with_bias,initial_weights)\n",
    "label_general_test = [label_dict[label] for label in Mnist_df_test_data_with_bias[:,-1]]\n",
    "label_general_test = np.asarray(label_general_test)\n",
    "prediction_test = np.argmax(softmax_output_test, axis=1)\n",
    "print (prediction_test,label_general_test)\n",
    "print ('Testing_accuracy: %.2f' % (np.mean(prediction_test == label_general_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
