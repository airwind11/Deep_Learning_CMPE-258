{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import random\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mnist_df = pd.read_csv(\"ex3_train.csv\")\n",
    "# Alternative way of doing -----Mnist_df_train_data = Mnist_df.as_matrix(columns=Mnist_df.columns[0:400])\n",
    "Mnist_df_train_data = Mnist_df.values\n",
    "Mnist_df_train_label = Mnist_df.as_matrix(columns=Mnist_df.columns[400:401])\n",
    "Mnist_df_train_data_with_bias = np.c_[np.ones((Mnist_df_train_data.shape[0] ,1)),Mnist_df_train_data]## Add 1 for the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mnist_df_test = pd.read_csv(\"ex3_test.csv\")\n",
    "Mnist_df_test_data = Mnist_df_test.values\n",
    "Mnist_df_test_label = Mnist_df_test.as_matrix(columns=Mnist_df.columns[400:401])\n",
    "Mnist_df_test_data_with_bias = np.c_[np.ones((Mnist_df_test_data.shape[0] ,1)),Mnist_df_test_data]## Add 1 for the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11461a080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(1,1))\n",
    "plt.imshow(np.reshape(Mnist_df_train_data[:,:-1][2],[20,20]).T,cmap='gray')\n",
    "print (Mnist_df_train_data[2][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Label set - containing unique labels\n",
    "Mnist_df_train_label_uq = set(Mnist_df.values[:,400].astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Not Being used for this assignment\n",
    "def hot_encoding_dict(label_set):\n",
    "    identity_matrix = np.identity(len(label_set)).astype(int)\n",
    "    hot_encoding_dict = {v:identity_matrix[:,i] for i,v in enumerate(label_set)}\n",
    "    return hot_encoding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Being used for this assignment\n",
    "def label_dict_gen(label_set): ## Useful for scaling if  huge number of labels (both number and string)\n",
    "    label_dict = {}\n",
    "    count = 0\n",
    "    for label in label_set:\n",
    "        label_dict[label]=count\n",
    "        count +=1\n",
    "    return label_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Not Being used for this assignment - better method exists for calculating the cross entropy loss\n",
    "## Nielsen Deep Learning applicable for number labels ,if input is string label enumerate the labels and then use the  \n",
    "## enumerated index below \n",
    "def hot_encoding_lower_memory_footprint(label,number_of_labels):\n",
    "    e = np.zeros((number_of_labels, 1))\n",
    "    e[label] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Not Being used for this assignment - better method exists for calculating the cross entropy loss\n",
    "## Refer - corect_logprobs = -np.log(softmax_output[range(softmax_output.shape[0]),label_general])## Andrej Karpathy Course\n",
    "def prepare_true_label_hot_encoding_array(label_dict,labels):\n",
    "    label_list = []\n",
    "    for label in labels:\n",
    "        label_list.append(hot_encoding_lower_memory_footprint(label_dict[label],len(label_dict)))\n",
    "    label_list = np.array(label_list)\n",
    "    return label_list.reshape(-1,len(label_dict)).T\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print (prepare_true_label_hot_encoding_array(label_dict(Mnist_df_train_label_uq),[0,0,1,9,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(data):\n",
    "    return 1/(1+np.exp(-data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_prediction(data):\n",
    "    result= np.exp(data) / np.sum(np.exp(data), axis=1, keepdims=True)\n",
    "    return result ## The max proability per coloumn(each coloumn represents a record , and \n",
    "                                         ## The complete result matrix to calculate the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize weights along with weights for bias set to zero\n",
    "def weights_list_generator(features,hidden_layers_list,no_output_class):\n",
    "    np.random.seed(1)\n",
    "    weights_array=[]\n",
    "    for i in range(len(hidden_layers_list)):\n",
    "        if i == 0 :\n",
    "            weights_array=[.01*np.random.randn(features,hidden_layers_list[0])]\n",
    "           ## Add row of zeroes for bias term's initial weights (initialized to zero)\n",
    "            weights_array[0] = np.r_[np.zeros(( 1,hidden_layers_list[0])),weights_array[0]]\n",
    "        else :\n",
    "            weights_array.append(.01*np.random.randn(hidden_layers_list[i-1],hidden_layers_list[i]))\n",
    "            ## Add row of zeroes for bias term's initial weights (initialized to zero)\n",
    "            weights_array[i] = np.r_[np.zeros(( 1,hidden_layers_list[i])),weights_array[i]]\n",
    "    weights_array.append(.01*np.random.randn(hidden_layers_list[-1],no_output_class))\n",
    "    weights_array[-1] = np.r_[np.zeros(( 1,no_output_class)),weights_array[-1]]\n",
    "    return weights_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights_list_try = weights_list_generator(400,[25],10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights_list_try[1][1:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights_list_try[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nweights_list = weights_list_generator(400,[25],10)\\nhidden_input = np.dot(Mnist_df_train_data_with_bias[:,:-1],weights_list[0])\\nhidden_output = sigmoid(hidden_input)\\nhidden_output = np.c_[np.ones((hidden_output.shape[0] ,1)),hidden_output]\\n#print ((1-hidden_output))\\n#print ((1-hidden_output[:,1:]))\\n#print (hidden_output[:,1:].shape)\\noutput_before_softmax = np.dot(hidden_output,weights_list[1])\\n#print(output_before_softmax)\\n#print(output_before_softmax.shape)\\nsoftmax_output = softmax_prediction(output_before_softmax)\\n#print(softmax_output)\\n#print(softmax_output.shape)\\nlabel_dict = label_dict_gen(Mnist_df_train_label_uq)\\nlabel_general = [label_dict[label] for label in Mnist_df_train_data_with_bias[:,-1]]\\nlabel_general = np.asarray(label_general)\\ncorect_logprobs = -np.log(softmax_output[range(softmax_output.shape[0]),label_general])\\n#print (corect_logprobs)\\n#print (corect_logprobs.shape)\\n## Sum the individual terms of output of autoencoding snd softmax in the above step and average it\\nloss = np.sum(corect_logprobs)/softmax_output.shape[0]    \\n\\n# compute the gradient on scores\\ndsoftmax = softmax_output\\ndsoftmax[range(softmax_output.shape[0]),label_general] -= 1 ## Very elegant only the element with true label is updated\\n#print (dsoftmax)\\n#print (dsoftmax.shape)\\n# backpropate the gradient to the 2nd layer weight\\ndW1 = np.dot(hidden_output.T, dsoftmax)/softmax_output.shape[0]\\n# perform a parameter update\\n#print (dW1)\\n#print (dW1.shape)\\n#print (weights_list[1])\\ngradient_descent(weights_list[1],dW1,.01)\\n#print (weights_list[1])\\n#print (weights_list[1].shape)\\n# Backpropagate the gradient to 1st layer \\nprint (weights_list[1][1:,:].shape)\\nyo = np.dot(dsoftmax,weights_list[1][1:,:].T)\\n#print(yo)\\n#print(yo.shape)\\npo = np.dot(Mnist_df_train_data_with_bias[:,:-1].T,np.dot(hidden_output[:,1:],(1-hidden_output)[:,1:].T))\\n#print (po)\\n#print (po.shape)\\ndW0 = np.dot(po,yo)/softmax_output.shape[0]\\n#print (dW0)\\n#print (dW0.shape)\\n# perform a parameter update\\ngradient_descent(weights_list[0],dW0,.01)\\nprint(weights_list[0])\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "weights_list = weights_list_generator(400,[25],10)\n",
    "hidden_input = np.dot(Mnist_df_train_data_with_bias[:,:-1],weights_list[0])\n",
    "hidden_output = sigmoid(hidden_input)\n",
    "hidden_output = np.c_[np.ones((hidden_output.shape[0] ,1)),hidden_output]\n",
    "#print ((1-hidden_output))\n",
    "#print ((1-hidden_output[:,1:]))\n",
    "#print (hidden_output[:,1:].shape)\n",
    "output_before_softmax = np.dot(hidden_output,weights_list[1])\n",
    "#print(output_before_softmax)\n",
    "#print(output_before_softmax.shape)\n",
    "softmax_output = softmax_prediction(output_before_softmax)\n",
    "#print(softmax_output)\n",
    "#print(softmax_output.shape)\n",
    "label_dict = label_dict_gen(Mnist_df_train_label_uq)\n",
    "label_general = [label_dict[label] for label in Mnist_df_train_data_with_bias[:,-1]]\n",
    "label_general = np.asarray(label_general)\n",
    "corect_logprobs = -np.log(softmax_output[range(softmax_output.shape[0]),label_general])\n",
    "#print (corect_logprobs)\n",
    "#print (corect_logprobs.shape)\n",
    "## Sum the individual terms of output of autoencoding snd softmax in the above step and average it\n",
    "loss = np.sum(corect_logprobs)/softmax_output.shape[0]    \n",
    "\n",
    "# compute the gradient on scores\n",
    "dsoftmax = softmax_output\n",
    "dsoftmax[range(softmax_output.shape[0]),label_general] -= 1 ## Very elegant only the element with true label is updated\n",
    "#print (dsoftmax)\n",
    "#print (dsoftmax.shape)\n",
    "# backpropate the gradient to the 2nd layer weight\n",
    "dW1 = np.dot(hidden_output.T, dsoftmax)/softmax_output.shape[0]\n",
    "# perform a parameter update\n",
    "#print (dW1)\n",
    "#print (dW1.shape)\n",
    "#print (weights_list[1])\n",
    "gradient_descent(weights_list[1],dW1,.01)\n",
    "#print (weights_list[1])\n",
    "#print (weights_list[1].shape)\n",
    "# Backpropagate the gradient to 1st layer \n",
    "print (weights_list[1][1:,:].shape)\n",
    "yo = np.dot(dsoftmax,weights_list[1][1:,:].T)\n",
    "#print(yo)\n",
    "#print(yo.shape)\n",
    "po = np.dot(Mnist_df_train_data_with_bias[:,:-1].T,np.dot(hidden_output[:,1:],(1-hidden_output)[:,1:].T))\n",
    "#print (po)\n",
    "#print (po.shape)\n",
    "dW0 = np.dot(po,yo)/softmax_output.shape[0]\n",
    "#print (dW0)\n",
    "#print (dW0.shape)\n",
    "# perform a parameter update\n",
    "gradient_descent(weights_list[0],dW0,.01)\n",
    "print(weights_list[0])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(data,weights_list):\n",
    "    hidden_input = np.dot(data[:,:-1],weights_list[0])\n",
    "    hidden_output = sigmoid(hidden_input)\n",
    "    hidden_output = np.c_[np.ones((hidden_output.shape[0] ,1)),hidden_output]## Add 1 for the bias \n",
    "    output_before_softmax = np.dot(hidden_output,weights_list[1])\n",
    "    softmax_output = softmax_prediction(output_before_softmax)\n",
    "    return softmax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(softmax_output,labels):\n",
    "    ## General implementation to take care of any labels\n",
    "    label_general = [label_dict(Mnist_df_train_label_uq)[label] for label in labels]\n",
    "    label_general = np.asarray(label_general)\n",
    "    corect_logprobs = -np.log(softmax_output[range(softmax_output.shape[0]),label_general])## Andrej Karpathy Course\n",
    "    loss = np.sum(corect_logprobs)/softmax_output.shape[0]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Backward_prop(data,initial_weights,label_dict,learning_rate):\n",
    "    hidden_input = np.dot(data[:,:-1],initial_weights[0])\n",
    "    hidden_output = sigmoid(hidden_input)\n",
    "    hidden_output = np.c_[np.ones((hidden_output.shape[0] ,1)),hidden_output]## Add 1 for the bias\n",
    "    #print (\"at_back\",data[:,-1])\n",
    "    output_before_softmax = np.dot(hidden_output,initial_weights[1])\n",
    "    softmax_output = softmax_prediction(output_before_softmax)\n",
    "    label_general = [label_dict[label] for label in data[:,-1]]\n",
    "    label_general = np.asarray(label_general)\n",
    "    #print (\"label\",label_general)\n",
    "    ## Andrej Karpathy Course calcualting the product of softmax with autoencodin to give the cross-entropy array                 \n",
    "    corect_logprobs = -np.log(softmax_output[range(softmax_output.shape[0]),label_general])\n",
    "    ## Sum the individual terms of output of autoencoding snd softmax in the above step and average it\n",
    "    loss = np.sum(corect_logprobs)/softmax_output.shape[0]    \n",
    "    \n",
    "  # compute the gradient on scores\n",
    "    dsoftmax = softmax_output\n",
    "    dsoftmax[range(softmax_output.shape[0]),label_general] -= 1 ## Very elegant only the element with true label is updated\n",
    "\n",
    "  # backpropate the gradient to the 2nd layer weight\n",
    "    dW1 = np.dot(hidden_output.T, dsoftmax)/softmax_output.shape[0]\n",
    "  # perform a parameter update\n",
    "    gradient_descent(initial_weights[1],dW1,learning_rate)\n",
    "    \n",
    "    dlast = np.dot(dsoftmax,initial_weights[1][1:,:].T)\n",
    "    dsigmoid = np.dot(hidden_output[:,1:],(1-hidden_output[:,1:]).T)\n",
    "    \n",
    "    dhidden = np.dot(dsigmoid,dlast)\n",
    "  # Backpropagate the gradient to 1st layer weights\n",
    "    dW0 = np.dot(data[:,:-1].T,dhidden)/softmax_output.shape[0]\n",
    "  # perform a parameter update\n",
    "    gradient_descent(initial_weights[0],dW0,learning_rate)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(weights,gradient,learning_rate):\n",
    "    weights += -learning_rate * gradient\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(training_data, epochs, mini_batch_size,initial_weights,Loss_graph,learning_rate = .0001,test_data=None):\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = training_data.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            np.random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k+mini_batch_size]for k in range(0, n, mini_batch_size)]\n",
    "            #print (len(mini_batches))\n",
    "            for mini_batch in mini_batches:\n",
    "                ## Loss gets updated with each batch and after the loop ends , the loss reflects the loss at end of 1 epoch\n",
    "                Loss = Backward_prop(mini_batch,initial_weights,label_dict,learning_rate)\n",
    "                #print (mini_batch[:,-1])\n",
    "                #break\n",
    "            #break\n",
    "            if epoch % 1000== 0:\n",
    "                Loss_graph[learning_rate].append([Loss,epoch])\n",
    "                print (\"iteration %d: loss %f\" % (epoch, Loss))\n",
    "                softmax_output = forward_prop(training_data,initial_weights)\n",
    "                prediction = np.argmax(softmax_output, axis=1)\n",
    "                label_general = [label_dict_gen(Mnist_df_train_label_uq)[label] for label in training_data[:,-1]]\n",
    "                label_general = np.asarray(label_general)\n",
    "                print(prediction,label_general)\n",
    "                accuracy = np.mean(prediction == label_general)\n",
    "                print ('training accuracy: %.4f' % accuracy)\n",
    "                if accuracy>0.95  :\n",
    "                    break\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MINI BATCHING APPROACH\n",
    "# output_batches = []\n",
    "# sample_size = len(features)\n",
    "# for start_i in range(0, sample_size, batch_size):\n",
    "#         end_i = start_i + batch_size\n",
    "#         batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "#         output_batches.append(batch)\n",
    "#     return output_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights = weights_list_generator(400,[25],10)\n",
    "label_dict = label_dict_gen(Mnist_df_train_label_uq)\n",
    "Loss_graph = defaultdict(list)\n",
    "mini_batch_size = 3500\n",
    "epochs = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 2.303443\n",
      "[1 1 1 ... 1 1 1] [4 2 6 ... 3 4 7]\n",
      "training accuracy: 0.0997\n",
      "iteration 1000: loss 2.292892\n",
      "[8 8 2 ... 2 2 2] [2 8 2 ... 2 3 3]\n",
      "training accuracy: 0.1960\n",
      "iteration 2000: loss 2.274288\n",
      "[8 0 2 ... 2 1 2] [3 2 6 ... 4 6 4]\n",
      "training accuracy: 0.4246\n",
      "iteration 3000: loss 2.245733\n",
      "[1 2 9 ... 1 2 4] [1 8 4 ... 3 6 4]\n",
      "training accuracy: 0.5680\n",
      "iteration 4000: loss 2.225686\n",
      "[2 1 1 ... 3 7 6] [8 5 5 ... 5 7 6]\n",
      "training accuracy: 0.6771\n",
      "iteration 5000: loss 2.178863\n",
      "[7 5 0 ... 9 3 0] [9 5 3 ... 9 3 0]\n",
      "training accuracy: 0.7289\n",
      "iteration 6000: loss 2.177279\n",
      "[1 3 3 ... 9 7 7] [1 3 3 ... 4 7 7]\n",
      "training accuracy: 0.7663\n",
      "iteration 7000: loss 2.119922\n",
      "[6 7 9 ... 7 1 1] [2 7 9 ... 7 5 1]\n",
      "training accuracy: 0.7934\n",
      "iteration 8000: loss 2.085881\n",
      "[2 6 1 ... 2 7 1] [2 6 1 ... 2 7 5]\n",
      "training accuracy: 0.8094\n",
      "iteration 9000: loss 2.083928\n",
      "[8 6 3 ... 2 1 4] [1 6 5 ... 6 0 4]\n",
      "training accuracy: 0.8177\n",
      "iteration 10000: loss 2.021811\n",
      "[6 7 3 ... 1 5 4] [6 7 3 ... 1 5 4]\n",
      "training accuracy: 0.8351\n",
      "iteration 11000: loss 1.996842\n",
      "[0 4 0 ... 8 8 2] [0 4 3 ... 8 8 2]\n",
      "training accuracy: 0.8431\n",
      "iteration 12000: loss 1.963948\n",
      "[3 0 0 ... 0 8 9] [3 0 0 ... 0 8 9]\n",
      "training accuracy: 0.8506\n",
      "iteration 13000: loss 1.912385\n",
      "[7 1 7 ... 1 7 2] [7 1 7 ... 1 7 2]\n",
      "training accuracy: 0.8589\n",
      "iteration 14000: loss 1.848149\n",
      "[7 0 4 ... 1 5 4] [7 0 4 ... 1 6 4]\n",
      "training accuracy: 0.8643\n",
      "iteration 15000: loss 1.826675\n",
      "[7 7 5 ... 8 6 1] [7 7 5 ... 8 6 1]\n",
      "training accuracy: 0.8720\n",
      "iteration 16000: loss 1.834846\n",
      "[8 9 9 ... 0 9 9] [8 9 9 ... 0 9 9]\n",
      "training accuracy: 0.8711\n",
      "iteration 17000: loss 1.839608\n",
      "[4 8 0 ... 8 1 9] [4 8 0 ... 8 1 9]\n",
      "training accuracy: 0.8706\n",
      "iteration 18000: loss 1.811636\n",
      "[2 6 7 ... 5 8 0] [2 6 7 ... 5 8 0]\n",
      "training accuracy: 0.8746\n",
      "iteration 19000: loss 1.820355\n",
      "[2 3 4 ... 4 2 9] [2 3 4 ... 4 2 7]\n",
      "training accuracy: 0.8743\n",
      "iteration 20000: loss 1.798500\n",
      "[9 6 6 ... 0 3 9] [9 6 6 ... 0 3 9]\n",
      "training accuracy: 0.8720\n",
      "iteration 21000: loss 1.759918\n",
      "[0 8 3 ... 0 7 8] [0 8 3 ... 0 7 5]\n",
      "training accuracy: 0.8703\n",
      "iteration 22000: loss 1.784014\n",
      "[8 0 3 ... 7 0 7] [8 0 3 ... 7 0 7]\n",
      "training accuracy: 0.8589\n",
      "iteration 23000: loss 1.824644\n",
      "[6 0 1 ... 9 5 9] [6 0 1 ... 9 5 9]\n",
      "training accuracy: 0.8566\n",
      "iteration 24000: loss 1.823277\n",
      "[6 7 7 ... 4 1 3] [6 7 7 ... 4 1 3]\n",
      "training accuracy: 0.8234\n",
      "iteration 25000: loss 1.953846\n",
      "[4 7 0 ... 8 8 2] [4 7 0 ... 0 5 1]\n",
      "training accuracy: 0.7634\n",
      "iteration 26000: loss 2.135890\n",
      "[8 6 2 ... 3 3 1] [5 6 2 ... 7 3 0]\n",
      "training accuracy: 0.4311\n",
      "iteration 27000: loss 2.151605\n",
      "[3 4 0 ... 0 5 2] [9 4 3 ... 0 5 3]\n",
      "training accuracy: 0.2977\n",
      "iteration 28000: loss 2.176108\n",
      "[4 0 0 ... 5 0 0] [4 0 7 ... 9 3 6]\n",
      "training accuracy: 0.2863\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-7a633b201caa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMnist_df_train_data_with_bias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitial_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mLoss_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-56-0f886738315c>\u001b[0m in \u001b[0;36mSGD\u001b[0;34m(training_data, epochs, mini_batch_size, initial_weights, Loss_graph, learning_rate, test_data)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmini_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmini_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0;31m## Loss gets updated with each batch and after the loop ends , the loss reflects the loss at end of 1 epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0mLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBackward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitial_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0;31m#print (mini_batch[:,-1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0;31m#break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-9ef1e720e018>\u001b[0m in \u001b[0;36mBackward_prop\u001b[0;34m(data, initial_weights, label_dict, learning_rate)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mBackward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitial_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mhidden_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitial_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mhidden_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mhidden_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden_output\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m## Add 1 for the bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#print (\"at_back\",data[:,-1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "SGD(Mnist_df_train_data_with_bias, epochs, mini_batch_size,initial_weights,Loss_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_output_test = forward_prop(Mnist_df_test_data_with_bias,initial_weights)\n",
    "label_general_test = [label_dict[label] for label in Mnist_df_test_data_with_bias[:,-1]]\n",
    "label_general_test = np.asarray(label_general_test)\n",
    "prediction_test = np.argmax(softmax_output_test, axis=1)\n",
    "print (prediction_test,label_general_test)\n",
    "print ('Testing_accuracy: %.2f' % (np.mean(prediction_test == label_general_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
